{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: IPO Filings Web Scraping and Data Processing\n",
    "\n",
    "**What's the total sum ($m) of 2023 filings that happenned of Fridays?**\n",
    "\n",
    "Re-use the [Code Snippet 1] example to get the data from web for this endpoint: https://stockanalysis.com/ipos/filings/\n",
    "Convert the 'Filing Date' to datetime(), 'Shares Offered' to float64 (if '-' is encountered, populate with NaNs).\n",
    "Define a new field 'Avg_price' based on the \"Price Range\", which equals to NaN if no price is specified, to the price (if only one number is provided), or to the average of 2 prices (if a range is given).\n",
    "You may be inspired by the function `extract_numbers()` in [Code Snippet 4], or you can write your own function to \"parse\" a string.\n",
    "Define a column \"Shares_offered_value\", which equals to \"Shares Offered\" * \"Avg_price\" (when both columns are defined; otherwise, it's NaN)\n",
    "\n",
    "Find the total sum in $m (millions of USD, closest INTEGER number) for all fillings during 2023, which happened on Fridays (`Date.dt.dayofweek()==4`). You should see 32 records in total, 24 of it is not null.\n",
    "\n",
    "(additional: you can read about [S-1 IPO filing](https://www.dfinsolutions.com/knowledge-hub/thought-leadership/knowledge-resources/what-s-1-ipo-filing) to understand the context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcata\\AppData\\Local\\Temp\\ipykernel_17740\\503092549.py:11: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  filings_dfs = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "#make the call to the endpoint\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "}\n",
    "\n",
    "url = \"https://stockanalysis.com/ipos/filings/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "filings_dfs = pd.read_html(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 329 entries, 0 to 328\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Filing Date     329 non-null    object\n",
      " 1   Symbol          329 non-null    object\n",
      " 2   Company Name    329 non-null    object\n",
      " 3   Price Range     329 non-null    object\n",
      " 4   Shares Offered  329 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 13.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# put the result into a dataframe\n",
    "filings = filings_dfs[0]\n",
    "filings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define `Filling Date` to datetime\n",
    "filings['Filing Date'] = pd.to_datetime(filings['Filing Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 329 entries, 0 to 328\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Filing Date     329 non-null    datetime64[ns]\n",
      " 1   Symbol          329 non-null    object        \n",
      " 2   Company Name    329 non-null    object        \n",
      " 3   Price Range     329 non-null    object        \n",
      " 4   Shares Offered  329 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 13.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# check the type\n",
    "filings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define `Shares Offered` as float with some cleaning\n",
    "filings['Shares Offered'] = pd.to_numeric(filings['Shares Offered'].str.replace('-', ''), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings['Shares Offered'] = pd.to_numeric(filings['Shares Offered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 329 entries, 0 to 328\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Filing Date     329 non-null    datetime64[ns]\n",
      " 1   Symbol          329 non-null    object        \n",
      " 2   Company Name    329 non-null    object        \n",
      " 3   Price Range     329 non-null    object        \n",
      " 4   Shares Offered  253 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(3)\n",
      "memory usage: 13.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# cheking the type of `Shares Offered`\n",
    "filings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['$10.00', '-', '$5.00', '$4.00 - $5.00', '$4.00 - $4.50',\n",
       "       '$5.00 - $6.00', '$4.00 - $6.00', '$5.00 - $7.00',\n",
       "       '$10.00 - $12.00', '$3.00 - $4.00', '$4.00', '$4.13',\n",
       "       '$10.00 - $14.00', '$3.50 - $4.50', '$7.00 - $8.00',\n",
       "       '$8.00 - $10.00', '$11.25 - $13.75', '$4.00 - $4.75', '$4.30',\n",
       "       '$9.00 - $11.00', '$3.00 - $5.00', '$4.50 - $5.50',\n",
       "       '$5.75 - $6.75', '$8.00 - $9.00', '$5.50', '$4.35 - $6.35',\n",
       "       '$20.00', '$6.00 - $6.50', '$6.00 - $7.00', '$6.00', '$4.45',\n",
       "       '$8.00', '$15.00', '$6.25', '$4.25 - $6.25', '$5.20 - $7.20',\n",
       "       '$4.50 - $6.50', '$4.25', '$7.00 - $9.00', '$5.75',\n",
       "       '$18.00 - $20.00', '$7.00 - $7.50', '$8.50 - $9.50',\n",
       "       '$5.00 - $6.50'], dtype=object)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking the `Price Range` to do the correct processing\n",
    "filings['Price Range'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.5', '15.5']\n",
      "Average Price: 13.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_avg_price(price_range):\n",
    "    # Regular expression pattern to extract numbers from the price range string\n",
    "    pattern = r'\\d+\\.\\d+?'\n",
    "    \n",
    "    # Find all numbers in the price range string\n",
    "    prices = re.findall(pattern, price_range)\n",
    "    # Convert numbers to float and handle cases where only one price is provided or no price is specified\n",
    "    if len(prices) == 0:\n",
    "        return np.nan\n",
    "    elif len(prices) == 1:\n",
    "        return float(prices[0])\n",
    "    else:\n",
    "        # Calculate the average of two prices if a range is given\n",
    "        return (float(prices[0]) + float(prices[1])) / 2\n",
    "\n",
    "# Example usage:\n",
    "price_range = \"10.5 - 15.5\"\n",
    "avg_price = calculate_avg_price(price_range)\n",
    "print(\"Average Price:\", avg_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.0']\n",
      "[]\n",
      "['5.0']\n",
      "['10.0']\n",
      "[]\n",
      "['5.0']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['4.0', '4.5']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['5.0', '6.0']\n",
      "[]\n",
      "['10.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['5.0', '7.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['10.0']\n",
      "['10.0', '12.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['3.0', '4.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['4.0']\n",
      "['4.0', '5.0']\n",
      "['4.0', '5.0']\n",
      "['4.1']\n",
      "[]\n",
      "[]\n",
      "['5.0']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['4.0', '5.0']\n",
      "['5.0', '6.0']\n",
      "[]\n",
      "['10.0', '14.0']\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "[]\n",
      "['10.0']\n",
      "['3.5', '4.5']\n",
      "[]\n",
      "['7.0', '8.0']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['8.0', '10.0']\n",
      "[]\n",
      "['11.2', '13.7']\n",
      "[]\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['10.0']\n",
      "[]\n",
      "['4.0']\n",
      "['5.0']\n",
      "['4.0', '5.0']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "[]\n",
      "['4.0']\n",
      "['10.0']\n",
      "['4.0', '4.7']\n",
      "['5.0']\n",
      "['3.5', '4.5']\n",
      "['8.0', '10.0']\n",
      "['4.0', '5.0']\n",
      "['4.0', '6.0']\n",
      "['4.3']\n",
      "['9.0', '11.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['5.0', '7.0']\n",
      "['4.0', '6.0']\n",
      "['10.0']\n",
      "['3.0', '5.0']\n",
      "['4.5', '5.5']\n",
      "[]\n",
      "['4.0']\n",
      "['4.0', '5.0']\n",
      "['5.7', '6.7']\n",
      "['4.0']\n",
      "['5.0', '7.0']\n",
      "[]\n",
      "[]\n",
      "['4.0']\n",
      "['5.0', '6.0']\n",
      "['4.0', '6.0']\n",
      "['8.0', '9.0']\n",
      "['5.0', '6.0']\n",
      "['4.0', '5.0']\n",
      "['10.0']\n",
      "['5.5']\n",
      "['4.0', '6.0']\n",
      "['4.0']\n",
      "[]\n",
      "['10.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['5.0']\n",
      "['5.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['4.3', '6.3']\n",
      "['10.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['4.0', '5.0']\n",
      "['4.0', '5.0']\n",
      "['5.0']\n",
      "['5.0']\n",
      "['20.0']\n",
      "['5.0', '6.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['4.5', '5.5']\n",
      "['4.0', '5.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['4.0', '5.0']\n",
      "['4.0', '5.0']\n",
      "['4.0', '6.0']\n",
      "['4.0', '5.0']\n",
      "['4.0']\n",
      "['4.0']\n",
      "['4.0', '5.0']\n",
      "['5.0']\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['10.0']\n",
      "['4.0', '5.0']\n",
      "['4.0']\n",
      "[]\n",
      "['6.0', '6.5']\n",
      "['4.0', '6.0']\n",
      "['10.0']\n",
      "['6.0', '7.0']\n",
      "['4.0', '5.0']\n",
      "['5.0', '7.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['4.0', '6.0']\n",
      "['6.0', '7.0']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['4.0', '5.0']\n",
      "['4.0', '6.0']\n",
      "['5.0', '6.0']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "['4.0']\n",
      "['6.0']\n",
      "['5.0']\n",
      "['5.0']\n",
      "['5.0']\n",
      "[]\n",
      "['10.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['4.0', '6.0']\n",
      "['4.4']\n",
      "['5.0']\n",
      "['4.0', '6.0']\n",
      "['8.0']\n",
      "['4.0', '6.0']\n",
      "['5.0', '6.0']\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['15.0']\n",
      "['6.0']\n",
      "['5.0']\n",
      "['10.0']\n",
      "['4.1']\n",
      "['4.0', '5.0']\n",
      "['4.0', '6.0']\n",
      "['6.2']\n",
      "['10.0']\n",
      "['5.0', '6.0']\n",
      "['4.2', '6.2']\n",
      "['5.0', '6.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "['4.0']\n",
      "['4.0', '6.0']\n",
      "['5.0']\n",
      "['5.2', '7.2']\n",
      "['4.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "['10.0']\n",
      "[]\n",
      "['10.0']\n",
      "['4.5', '6.5']\n",
      "['4.0', '5.0']\n",
      "['4.0', '5.0']\n",
      "[]\n",
      "['5.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['4.5', '5.5']\n",
      "['4.0', '6.0']\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['10.0']\n",
      "[]\n",
      "[]\n",
      "['4.0', '4.5']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['4.2']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['5.0', '7.0']\n",
      "['10.0']\n",
      "[]\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['7.0', '9.0']\n",
      "['10.0']\n",
      "['5.7']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['5.0']\n",
      "['6.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "[]\n",
      "['10.0']\n",
      "['10.0']\n",
      "['18.0', '20.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "[]\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['4.0', '6.0']\n",
      "['10.0']\n",
      "[]\n",
      "['10.0']\n",
      "['10.0']\n",
      "['5.5']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['10.0']\n",
      "['7.0', '7.5']\n",
      "[]\n",
      "['8.5', '9.5']\n",
      "[]\n",
      "[]\n",
      "['4.0', '6.0']\n",
      "[]\n",
      "['8.0', '10.0']\n",
      "['10.0']\n",
      "['8.0', '10.0']\n",
      "['4.0']\n",
      "['5.0', '6.5']\n"
     ]
    }
   ],
   "source": [
    "# Apply the func to create `Avg_price`\n",
    "filings['Avg_price'] = filings['Price Range'].apply(calculate_avg_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `Shares_offered_value` with a lambda function \n",
    "filings['Shares_offered_value'] = filings.apply(lambda row: row['Shares Offered'] * row['Avg_price'] if row['Shares Offered'] != np.nan and row['Avg_price'] != np.nan else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum in millions of USD for filings on Fridays in 2023: 276\n"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames in filings into a single DataFrame\n",
    "combined_df = pd.concat([filings], ignore_index=True)\n",
    "\n",
    "# Filter records for 2023 and Fridays\n",
    "fridays_2023_df = combined_df[(combined_df['Filing Date'].dt.year == 2023) & (combined_df['Filing Date'].dt.dayofweek == 4)]\n",
    "# Calculate the total sum in millions of USD\n",
    "total_sum_millions_usd = fridays_2023_df['Shares_offered_value'].sum()/1e6  # Convert to millions of USD\n",
    "\n",
    "# Round the total sum to the closest integer\n",
    "total_sum_millions_usd_rounded = round(total_sum_millions_usd)\n",
    "\n",
    "print(\"Total sum in millions of USD for filings on Fridays in 2023:\", total_sum_millions_usd_rounded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:  IPOs \"Fixed days hold\" strategy\n",
    "\n",
    "\n",
    "**Find the optimal number of days X (between 1 and 30), where 75% quantile growth is the highest?**\n",
    "\n",
    "\n",
    "Reuse [Code Snippet 1] to retrieve the list of IPOs from 2023 and 2024 (from URLs: https://stockanalysis.com/ipos/2023/ and https://stockanalysis.com/ipos/2024/). \n",
    "Get all OHLCV daily prices for all stocks with an \"IPO date\" before March 1, 2024 (\"< 2024-03-01\") - 184 tickers (without 'RYZB'). Please remove 'RYZB', as it is no longer available on Yahoo Finance. \n",
    "\n",
    "Sometimes you may need to adjust the symbol name (e.g., 'IBAC' on stockanalysis.com -> 'IBACU' on Yahoo Finance) to locate OHLCV prices for all stocks.\n",
    "Some of the tickers like 'DYCQ' and 'LEGT' were on the market less than 30 days (11 and 21 days, respectively). Let's leave them in the dataset; it just means that you couldn't hold them for more days than they were listed.\n",
    "\n",
    "Let's assume you managed to buy a new stock (listed on IPO) on the first day at the [Adj Close] price]. Your strategy is to hold for exactly X full days (where X is between 1 and 30) and sell at the \"Adj. Close\" price in X days (e.g., if X=1, you sell on the next day).\n",
    "Find X, when the 75% quantile growth (among 185 investments) is the highest. \n",
    "\n",
    "HINTs:\n",
    "* You can generate 30 additional columns: growth_future_1d ... growth_future_30d, join that with the table of min_dates (first day when each stock has data on Yahoo Finance), and perform vector operations on the resulting dataset.\n",
    "* You can use the `DataFrame.describe()` function to get mean, min, max, 25-50-75% quantiles.\n",
    "\n",
    "\n",
    "Addtional: \n",
    "* You can also ensure that the mean and 50th percentile (median) investment returns are negative for most X values, implying a wager for a \"lucky\" investor who might be in the top 25%.\n",
    "* What's your recommendation: Do you suggest pursuing this strategy for an optimal X?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcata\\AppData\\Local\\Temp\\ipykernel_17740\\3740329921.py:8: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  ipo_dfs = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "}\n",
    "\n",
    "url = \"https://stockanalysis.com/ipos/2023/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "ipo_dfs = pd.read_html(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154 entries, 0 to 153\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   IPO Date      154 non-null    object\n",
      " 1   Symbol        154 non-null    object\n",
      " 2   Company Name  154 non-null    object\n",
      " 3   IPO Price     154 non-null    object\n",
      " 4   Current       154 non-null    object\n",
      " 5   Return        154 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "ipos_2023 = ipo_dfs[0]\n",
    "ipos_2023.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rcata\\AppData\\Local\\Temp\\ipykernel_17740\\1522156903.py:4: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  ipo_dfs = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://stockanalysis.com/ipos/2024/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "ipo_dfs = pd.read_html(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63 entries, 0 to 62\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   IPO Date      63 non-null     object\n",
      " 1   Symbol        63 non-null     object\n",
      " 2   Company Name  63 non-null     object\n",
      " 3   IPO Price     63 non-null     object\n",
      " 4   Current       63 non-null     object\n",
      " 5   Return        63 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "ipos_2024 = ipo_dfs[0]\n",
    "ipos_2024.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of stacked IPOs DataFrame:\n",
      "       IPO Date Symbol                   Company Name IPO Price Current  \\\n",
      "0  Dec 27, 2023   IROH  Iron Horse Acquisitions Corp.    $10.00  $10.03   \n",
      "1  Dec 19, 2023   LGCB             Linkage Global Inc     $4.00   $3.14   \n",
      "2  Dec 15, 2023    ZKH              ZKH Group Limited    $15.50  $12.50   \n",
      "3  Dec 15, 2023   BAYA       Bayview Acquisition Corp    $10.00  $10.18   \n",
      "4  Dec 14, 2023   INHD             Inno Holdings Inc.     $4.00   $0.71   \n",
      "\n",
      "    Return  \n",
      "0    0.30%  \n",
      "1  -19.00%  \n",
      "2  -18.71%  \n",
      "3    1.80%  \n",
      "4  -82.25%  \n"
     ]
    }
   ],
   "source": [
    "# Concatenate IPO DataFrames for 2023 and 2024\n",
    "stacked_ipos_df = pd.concat([ipos_2023, ipos_2024], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the concatenated DataFrame\n",
    "print(\"First few rows of stacked IPOs DataFrame:\")\n",
    "print(stacked_ipos_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPO Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>IPO Price</th>\n",
       "      <th>Current</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dec 27, 2023</td>\n",
       "      <td>IROH</td>\n",
       "      <td>Iron Horse Acquisitions Corp.</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>$10.03</td>\n",
       "      <td>0.30%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IPO Date Symbol                   Company Name IPO Price Current Return\n",
       "0  Dec 27, 2023   IROH  Iron Horse Acquisitions Corp.    $10.00  $10.03  0.30%"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_ipos_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "stacked_ipos_df['IPO Date'] = pd.to_datetime(stacked_ipos_df['IPO Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPO Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>IPO Price</th>\n",
       "      <th>Current</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [IPO Date, Symbol, Company Name, IPO Price, Current, Return]\n",
       "Index: []"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem --> not always the columns are filled\n",
    "missing_prices_df = stacked_ipos_df[stacked_ipos_df['IPO Price'].astype(str).str.find('-') >= 0]\n",
    "missing_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it has some missing values --> use defensive errors='coerce' (if don't have time to crack into the data errors)\n",
    "#     : pd.to_numeric() function call, which will convert problematic values to NaN.\n",
    "#     otherwise you'll get a ValueError: Unable to parse string \"-\" at position 9\n",
    "stacked_ipos_df['IPO Price'] = pd.to_numeric(stacked_ipos_df['IPO Price'].str.replace('$', ''), errors='coerce')\n",
    "# not sure why, but need to call it again to transform 'object' to 'float64'\n",
    "stacked_ipos_df['IPO Price'] = pd.to_numeric(stacked_ipos_df['IPO Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Current\" column\n",
    "stacked_ipos_df['Current'] = pd.to_numeric(stacked_ipos_df['Current'].str.replace('$', ''), errors='coerce')\n",
    "\n",
    "# Convert 'Return' to numeric format (percentage)\n",
    "stacked_ipos_df['Return'] = pd.to_numeric(stacked_ipos_df['Return'].str.replace('%', ''), errors='coerce') / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217 entries, 0 to 216\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   IPO Date      217 non-null    datetime64[ns]\n",
      " 1   Symbol        217 non-null    object        \n",
      " 2   Company Name  217 non-null    object        \n",
      " 3   IPO Price     217 non-null    float64       \n",
      " 4   Current       217 non-null    float64       \n",
      " 5   Return        216 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(2)\n",
      "memory usage: 10.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Correctly applied transformations with 'defensive' techniques, but now not all are non-null\n",
    "stacked_ipos_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPO Date        0\n",
       "Symbol          0\n",
       "Company Name    0\n",
       "IPO Price       0\n",
       "Current         0\n",
       "Return          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple way of checking NULLs\n",
    "# (you need to understand how vector operations work .isnull() and calls chaining .isnull().sum())\n",
    "stacked_ipos_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPO Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>IPO Price</th>\n",
       "      <th>Current</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>MRX</td>\n",
       "      <td>Marex Group plc</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      IPO Date Symbol     Company Name  IPO Price  Current  Return\n",
       "157 2024-04-25    MRX  Marex Group plc       19.0     19.1     NaN"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do you want to leave the record or not?\n",
    "stacked_ipos_df[stacked_ipos_df.Return.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stacked_ipos_df['Symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = stacked_ipos_df[stacked_ipos_df['IPO Date'] < '2024-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IROH', 'LGCB', 'ZKH', 'BAYA', 'INHD', 'AFJK', 'GSIW', 'FEBO',\n",
       "       'CLBR', 'ELAB', 'RR', 'DDC', 'SHIM', 'GLAC', 'SGN', 'HG', 'CRGX',\n",
       "       'ANSC', 'AITR', 'GVH', 'LXEO', 'PAPL', 'ATGL', 'MNR', 'WBUY',\n",
       "       'NCL', 'BIRK', 'GMM', 'LRHC', 'PMEC', 'GPAK', 'SPKL', 'QETA',\n",
       "       'MSS', 'ANL', 'SYRA', 'VSME', 'LRE', 'TURB', 'MDBH', 'KVYO',\n",
       "       'CART', 'DTCK', 'RYZB', 'NMRA', 'ARM', 'SPPL', 'NWGL', 'SWIN',\n",
       "       'IVP', 'NNAG', 'SRM', 'SPGC', 'LQR', 'NRXS', 'FTEL', 'MIRA',\n",
       "       'PXDT', 'CTNT', 'HRYU', 'SRFM', 'PRZO', 'HYAC', 'KVAC', 'JNVR',\n",
       "       'ELWS', 'WRNT', 'TSBX', 'ODD', 'APGE', 'NETD', 'SGMT', 'BOWN',\n",
       "       'SXTP', 'PWM', 'VTMX', 'INTS', 'SVV', 'KGS', 'FIHL', 'GENK',\n",
       "       'BUJA', 'BOF', 'AZTR', 'CAVA', 'ESHA', 'ATMU', 'ATS', 'IPXX',\n",
       "       'CWD', 'SGE', 'SLRN', 'ALCY', 'KVUE', 'GODN', 'TRNR', 'AACT',\n",
       "       'JYD', 'USGO', 'UCAR', 'WLGS', 'TPET', 'TCJH', 'GDTC', 'VCIG',\n",
       "       'GDHG', 'ARBB', 'ISPR', 'MGIH', 'MWG', 'HSHP', 'SFWL', 'SYT',\n",
       "       'HKIT', 'CHSN', 'TBMC', 'HLP', 'ZJYL', 'TMTC', 'YGFGF', 'OAKU',\n",
       "       'BANL', 'OMH', 'MGRX', 'FORL', 'ICG', 'IZM', 'AESI', 'AIXI',\n",
       "       'SBXC', 'BMR', 'DIST', 'GXAI', 'MARX', 'BFRG', 'ENLT', 'MLYS',\n",
       "       'PTHR', 'BLAC', 'NXT', 'HSAI', 'LSDI', 'LICN', 'GPCR', 'ASST',\n",
       "       'CETU', 'TXO', 'BREA', 'GNLX', 'QSG', 'CVKD', 'SKWD', 'ISRL',\n",
       "       'MGOL', 'SMXT', 'VHAI', 'DYCQ', 'CHRO', 'UMAC', 'TBBB', 'MGX',\n",
       "       'HLXB', 'TELO', 'KYTX', 'PMNT', 'AHR', 'LEGT', 'ANRO', 'GUTS',\n",
       "       'AS', 'FBLG', 'BTSG', 'AVBP', 'HAO', 'CGON', 'YIBO', 'SUGP', 'JL',\n",
       "       'KSPI', 'JVSA', 'PSBD', 'CCTG', 'SYNX', 'SDHC', 'ROMA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['Symbol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'IBACU' in list(filtered_df['Symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df['Symbol'] != 'RYZB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df['Symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = filtered_df['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'IBACU' in tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['PTHR']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['DYCQ']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['LEGT']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['JVSA']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHLCV data for the first ticker (IROH):\n",
      "             Open   High     Low   Close  Adj Close  Volume\n",
      "Date                                                       \n",
      "2024-02-16  10.05  10.05  10.010  10.010     10.010   16700\n",
      "2024-02-20  10.02  10.02  10.020  10.020     10.020    5200\n",
      "2024-02-21  10.02  10.02  10.015  10.015     10.015   98600\n",
      "2024-02-22  10.02  10.02  10.020  10.020     10.020    5600\n",
      "2024-02-23  10.02  10.02  10.010  10.010     10.010   14800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Download OHLCV data for each ticker\n",
    "ohlcvs = {}\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        ohlcvs[ticker] = yf.download(ticker, start='2023-01-01', end='2024-03-01')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download data for ticker {ticker}: {str(e)}\")\n",
    "\n",
    "# Display the first few rows of OHLCV data for the first ticker\n",
    "if ohlcvs:\n",
    "    print(f\"OHLCV data for the first ticker ({tickers[0]}):\")\n",
    "    print(ohlcvs[tickers[0]].head())\n",
    "else:\n",
    "    print(\"No OHLCV data downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no data for ['PTHR', 'DYCQ', 'LEGT', 'JVSA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_growth_df(df:pd.DataFrame, prefix:str)->pd.DataFrame:\n",
    "  for i in [1,3,7,30,90,365]:\n",
    "    df['growth_'+prefix+'_'+str(i)+'d'] = df['Adj Close'] / df['Adj Close'].shift(i)\n",
    "    GROWTH_KEYS = [k for k in df.keys() if k.startswith('growth')]\n",
    "  return df[GROWTH_KEYS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Ticker   Min Date\n",
      "0     IROH 2024-02-16\n",
      "1     LGCB 2023-12-26\n",
      "2      ZKH 2023-12-15\n",
      "3     BAYA 2023-12-29\n",
      "4     INHD 2023-12-14\n",
      "..     ...        ...\n",
      "175   PSBD 2024-01-18\n",
      "176   CCTG 2024-01-18\n",
      "177   SYNX 2024-01-12\n",
      "178   SDHC 2024-01-16\n",
      "179   ROMA 2024-01-09\n",
      "\n",
      "[180 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store DataFrames for each ticker\n",
    "min_dates_list = []\n",
    "\n",
    "# Iterate through the DataFrame containing OHLCV data for each stock\n",
    "for ticker, df in ohlcvs.items():\n",
    "    # Check if the DataFrame is not empty\n",
    "    if not df.empty:\n",
    "        # Extract the minimum date for the current ticker\n",
    "        min_date = df.index.min()\n",
    "        # Create a DataFrame for the current ticker and its minimum date\n",
    "        min_date_df = pd.DataFrame({'Ticker': [ticker], 'Min Date': [min_date]})\n",
    "        # Append the DataFrame to the list\n",
    "        min_dates_list.append(min_date_df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "min_dates = pd.concat(min_dates_list, ignore_index=True)\n",
    "\n",
    "# Display the min_dates DataFrame\n",
    "print(min_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of X (number of days to hold the stock): 28\n",
      "Max 75th percentile growth: 0.03914195622091493\n"
     ]
    }
   ],
   "source": [
    "# Define a list to store the growth for each value of X\n",
    "growth_for_X = []\n",
    "\n",
    "# Iterate over each possible value of X (from 1 to 30)\n",
    "for X in range(1, 31):\n",
    "    # Calculate the growth for each investment based on the holding period of X days\n",
    "    growths = []\n",
    "    for ticker, df in ohlcvs.items():\n",
    "        # Calculate the percentage growth from buying on the first day and selling after X days\n",
    "        if len(df) >= X + 1:  # Ensure there are enough data points for X days\n",
    "            buy_price = df.iloc[0]['Adj Close']\n",
    "            sell_price = df.iloc[X]['Adj Close']\n",
    "            growth = (sell_price - buy_price) / buy_price\n",
    "            growths.append(growth)\n",
    "\n",
    "    # Calculate the 75th percentile growth for the current value of X\n",
    "    percentile_75 = np.percentile(growths, 75)\n",
    "    # Store the 75th percentile growth for the current value of X\n",
    "    growth_for_X.append(percentile_75)\n",
    "\n",
    "# Find the value of X that maximizes the 75th percentile growth\n",
    "optimal_X = np.argmax(growth_for_X) + 1  # Add 1 because indexing starts from 0\n",
    "max_growth = max(growth_for_X)\n",
    "\n",
    "print(\"Optimal value of X (number of days to hold the stock):\", optimal_X)\n",
    "print(\"Max 75th percentile growth:\", max_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       growth_future_1d  growth_future_2d  growth_future_3d  growth_future_4d  \\\n",
      "count               0.0               0.0               0.0               0.0   \n",
      "mean                NaN               NaN               NaN               NaN   \n",
      "min                 NaN               NaN               NaN               NaN   \n",
      "25%                 NaN               NaN               NaN               NaN   \n",
      "50%                 NaN               NaN               NaN               NaN   \n",
      "75%                 NaN               NaN               NaN               NaN   \n",
      "max                 NaN               NaN               NaN               NaN   \n",
      "std                 NaN               NaN               NaN               NaN   \n",
      "\n",
      "       growth_future_5d  growth_future_6d  growth_future_7d  growth_future_8d  \\\n",
      "count               0.0               0.0               0.0               0.0   \n",
      "mean                NaN               NaN               NaN               NaN   \n",
      "min                 NaN               NaN               NaN               NaN   \n",
      "25%                 NaN               NaN               NaN               NaN   \n",
      "50%                 NaN               NaN               NaN               NaN   \n",
      "75%                 NaN               NaN               NaN               NaN   \n",
      "max                 NaN               NaN               NaN               NaN   \n",
      "std                 NaN               NaN               NaN               NaN   \n",
      "\n",
      "       growth_future_9d  growth_future_10d  ...  growth_future_22d  \\\n",
      "count               0.0                0.0  ...                0.0   \n",
      "mean                NaN                NaN  ...                NaN   \n",
      "min                 NaN                NaN  ...                NaN   \n",
      "25%                 NaN                NaN  ...                NaN   \n",
      "50%                 NaN                NaN  ...                NaN   \n",
      "75%                 NaN                NaN  ...                NaN   \n",
      "max                 NaN                NaN  ...                NaN   \n",
      "std                 NaN                NaN  ...                NaN   \n",
      "\n",
      "       growth_future_23d  growth_future_24d  growth_future_25d  \\\n",
      "count                0.0                0.0                0.0   \n",
      "mean                 NaN                NaN                NaN   \n",
      "min                  NaN                NaN                NaN   \n",
      "25%                  NaN                NaN                NaN   \n",
      "50%                  NaN                NaN                NaN   \n",
      "75%                  NaN                NaN                NaN   \n",
      "max                  NaN                NaN                NaN   \n",
      "std                  NaN                NaN                NaN   \n",
      "\n",
      "       growth_future_26d  growth_future_27d  growth_future_28d  \\\n",
      "count                0.0                0.0                0.0   \n",
      "mean                 NaN                NaN                NaN   \n",
      "min                  NaN                NaN                NaN   \n",
      "25%                  NaN                NaN                NaN   \n",
      "50%                  NaN                NaN                NaN   \n",
      "75%                  NaN                NaN                NaN   \n",
      "max                  NaN                NaN                NaN   \n",
      "std                  NaN                NaN                NaN   \n",
      "\n",
      "       growth_future_29d  growth_future_30d  Min Date  \n",
      "count                0.0                0.0         0  \n",
      "mean                 NaN                NaN       NaT  \n",
      "min                  NaN                NaN       NaT  \n",
      "25%                  NaN                NaN       NaT  \n",
      "50%                  NaN                NaN       NaT  \n",
      "75%                  NaN                NaN       NaT  \n",
      "max                  NaN                NaN       NaT  \n",
      "std                  NaN                NaN       NaN  \n",
      "\n",
      "[8 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a DataFrame to store the growth for each investment over future holding periods\n",
    "growth_df = pd.DataFrame(index=ohlcvs.keys())\n",
    "\n",
    "# Calculate growth for each holding period (1 day to 30 days)\n",
    "for i in range(1, 31):\n",
    "    growth_df[f\"growth_future_{i}d\"] = [((ohlcvs[ticker].iloc[i]['Adj Close'] - ohlcvs[ticker].iloc[0]['Adj Close']) / ohlcvs[ticker].iloc[0]['Adj Close']) if len(ohlcvs[ticker]) > i else np.nan for ticker in growth_df.index]\n",
    "\n",
    "# Join growth_df with the table of min_dates\n",
    "merged_df = pd.merge(growth_df, min_dates, left_index=True, right_index=True)\n",
    "\n",
    "# Perform vector operations on the resulting dataset\n",
    "# For example, calculate the mean, min, max, and quantiles\n",
    "statistics = merged_df.describe()\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       growth_future_1d  growth_future_2d  growth_future_3d  growth_future_4d  \\\n",
      "count               0.0               0.0               0.0               0.0   \n",
      "mean                NaN               NaN               NaN               NaN   \n",
      "min                 NaN               NaN               NaN               NaN   \n",
      "25%                 NaN               NaN               NaN               NaN   \n",
      "50%                 NaN               NaN               NaN               NaN   \n",
      "75%                 NaN               NaN               NaN               NaN   \n",
      "max                 NaN               NaN               NaN               NaN   \n",
      "std                 NaN               NaN               NaN               NaN   \n",
      "\n",
      "       growth_future_5d  growth_future_6d  growth_future_7d  growth_future_8d  \\\n",
      "count               0.0               0.0               0.0               0.0   \n",
      "mean                NaN               NaN               NaN               NaN   \n",
      "min                 NaN               NaN               NaN               NaN   \n",
      "25%                 NaN               NaN               NaN               NaN   \n",
      "50%                 NaN               NaN               NaN               NaN   \n",
      "75%                 NaN               NaN               NaN               NaN   \n",
      "max                 NaN               NaN               NaN               NaN   \n",
      "std                 NaN               NaN               NaN               NaN   \n",
      "\n",
      "       growth_future_9d  growth_future_10d  ...  growth_future_22d  \\\n",
      "count               0.0                0.0  ...                0.0   \n",
      "mean                NaN                NaN  ...                NaN   \n",
      "min                 NaN                NaN  ...                NaN   \n",
      "25%                 NaN                NaN  ...                NaN   \n",
      "50%                 NaN                NaN  ...                NaN   \n",
      "75%                 NaN                NaN  ...                NaN   \n",
      "max                 NaN                NaN  ...                NaN   \n",
      "std                 NaN                NaN  ...                NaN   \n",
      "\n",
      "       growth_future_23d  growth_future_24d  growth_future_25d  \\\n",
      "count                0.0                0.0                0.0   \n",
      "mean                 NaN                NaN                NaN   \n",
      "min                  NaN                NaN                NaN   \n",
      "25%                  NaN                NaN                NaN   \n",
      "50%                  NaN                NaN                NaN   \n",
      "75%                  NaN                NaN                NaN   \n",
      "max                  NaN                NaN                NaN   \n",
      "std                  NaN                NaN                NaN   \n",
      "\n",
      "       growth_future_26d  growth_future_27d  growth_future_28d  \\\n",
      "count                0.0                0.0                0.0   \n",
      "mean                 NaN                NaN                NaN   \n",
      "min                  NaN                NaN                NaN   \n",
      "25%                  NaN                NaN                NaN   \n",
      "50%                  NaN                NaN                NaN   \n",
      "75%                  NaN                NaN                NaN   \n",
      "max                  NaN                NaN                NaN   \n",
      "std                  NaN                NaN                NaN   \n",
      "\n",
      "       growth_future_29d  growth_future_30d  Min Date  \n",
      "count                0.0                0.0         0  \n",
      "mean                 NaN                NaN       NaT  \n",
      "min                  NaN                NaN       NaT  \n",
      "25%                  NaN                NaN       NaT  \n",
      "50%                  NaN                NaN       NaT  \n",
      "75%                  NaN                NaN       NaT  \n",
      "max                  NaN                NaN       NaT  \n",
      "std                  NaN                NaN       NaN  \n",
      "\n",
      "[8 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a DataFrame to store the growth for each investment over future holding periods\n",
    "growth_df = pd.DataFrame(index=ohlcvs.keys())\n",
    "\n",
    "# Calculate growth for each holding period (1 day to 30 days)\n",
    "for i in range(1, 31):\n",
    "    # Calculate growth as before, but multiply by -1 to ensure negative returns\n",
    "    growth_df[f\"growth_future_{i}d\"] = [((ohlcvs[ticker].iloc[i]['Adj Close'] - ohlcvs[ticker].iloc[0]['Adj Close']) / ohlcvs[ticker].iloc[0]['Adj Close']) * -1 if len(ohlcvs[ticker]) > i else np.nan for ticker in growth_df.index]\n",
    "\n",
    "# Join growth_df with the table of min_dates\n",
    "merged_df = pd.merge(growth_df, min_dates, left_index=True, right_index=True)\n",
    "\n",
    "# Perform vector operations on the resulting dataset\n",
    "# For example, calculate the mean, min, max, and quantiles\n",
    "statistics = merged_df.describe()\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Is Growth Concentrated in the Largest Stocks?\n",
    "\n",
    "**Get the share of days (percentage as int) when Large Stocks outperform (growth_7d - growth over 7 periods back) the Largest stocks?**\n",
    "\n",
    "\n",
    "Reuse [Code Snippet 5] to obtain OHLCV stats for 33 stocks \n",
    "for 10 full years of data (2014-01-01 to 2023-12-31):\n",
    "\n",
    "`US_STOCKS = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'AVGO','V', 'JPM']`\n",
    "\n",
    "`EU_STOCKS = ['NVO','MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'SAP', 'ACN', 'TTE', 'SIE.DE','IDEXY','CDI.PA']`\n",
    "\n",
    "`INDIA_STOCKS = ['RELIANCE.NS','TCS.NS','HDB','BHARTIARTL.NS','IBN','SBIN.NS','LICI.NS','INFY','ITC.NS','HINDUNILVR.NS','LT.NS']`\n",
    "\n",
    "`LARGEST_STOCKS = US_STOCKS + EU_STOCKS + INDIA_STOCKS`\n",
    "<br/>\n",
    "\n",
    "Now let's add the top 12-22 stocks (as of end-April 2024):\n",
    "<br/>\n",
    "\n",
    "`NEW_US = ['TSLA','WMT','XOM','UNH','MA','PG','JNJ','MRK','HD','COST','ORCL']`\n",
    "\n",
    "`NEW_EU = ['PRX.AS','CDI.PA','AIR.PA','SU.PA','ETN','SNY','BUD','DTE.DE','ALV.DE','MDT','AI.PA','EL.PA']`\n",
    "\n",
    "`NEW_INDIA = ['BAJFINANCE.NS','MARUTI.NS','HCLTECH.NS','TATAMOTORS.NS','SUNPHARMA.NS','ONGC.NS','ADANIENT.NS','ADANIENT.NS','NTPC.NS','KOTAKBANK.NS','TITAN.NS']`\n",
    "\n",
    "`LARGE_STOCKS = NEW_EU + NEW_US + NEW_INDIA`\n",
    "\n",
    "You should be able to obtain stats for 33 LARGEST STOCKS and 32 LARGE STOCKS.\n",
    "\n",
    "Calculate  `growth_7d` for every stock and every day.\n",
    "Get the average daily `growth_7d` for the LARGEST_STOCKS group vs. the LARGE_STOCKS group.\n",
    "\n",
    "For example, for the first of data you should have:\n",
    "| Date   |      ticker_category      |  growth_7d |\n",
    "|----------|:-------------:|------:|\n",
    "| 2014-01-01 |  LARGE | 1.011684 |\n",
    "| 2014-01-01 |   LARGEST   |   1.011797 |\n",
    "\n",
    "On that day, the LARGEST group was growing faster than LARGE one (new stocks).\n",
    "\n",
    "Calculate the number of days when the LARGE GROUP (new smaller stocks) outperforms the LARGEST GROUP, divide it by the total number of trading days (which should be 2595 days), and convert it to a percentage (closest INTEGER value). For example, if you find that 1700 out of 2595 days meet this condition, it means that 1700/2595 = 0.655, or approximately 66% of days, the LARGE stocks were growing faster than the LARGEST ones. This suggests that you should consider extending your dataset with more stocks to seek higher growth.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Trying Another Technical Indicators strategy\n",
    "\n",
    "**What's the total gross profit (in THOUSANDS of $) you'll get from trading on CCI (no fees assumption)?**\n",
    "\n",
    "\n",
    "First, run the entire Colab to obtain the full DataFrame of data (after [Code Snippet 9]), and truncate it to the last full 10 years of data (2014-01-01 to 2023-12-31).\n",
    "If you encounter any difficulties running the Colab - you can download it using this [link](https://drive.google.com/file/d/1m3Qisfs2XfWk6Sw_Uk5kHLWqwQ0q8SKb/view?usp=sharing).\n",
    "\n",
    "Let's assume you've learned about the awesome **CCI indicator** ([Commodity Channel Index](https://www.investopedia.com/terms/c/commoditychannelindex.asp)), and decided to use only it for your operations.\n",
    "\n",
    "You defined the \"defensive\" value of a high threshould of 200, and you trade only on Fridays (`Date.dt.dayofweek()==4`).\n",
    "\n",
    "That is, every time you see that CCI is >200 for any stock (out of those 33), you'll invest $1000 (each record when CCI>200) at Adj.Close price and hold it for 1 week (5 trading days) in order to sell at the Adj. Close price.\n",
    "\n",
    "What's the expected gross profit (no fees) that you get in THOUSANDS $ (closest integer value) over many operations in 10 years?\n",
    "One operation calculations: if you invested $1000 and received $1010 in 5 days - you add $10 to gross profit, if you received $980 - add -$20 to gross profit.\n",
    "You need to sum these results over all trades (460 times in 10 years).\n",
    "\n",
    "Additional:\n",
    "  * Add an approximate fees calculation over the 460 trades from this calculator https://www.degiro.ie/fees/calculator (Product:\"Shares, USA and Canada;\" Amount per transaction: \"1000 EUR\"; Transactions per year: \"460\")\n",
    "  * are you still profitable on those trades?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [EXPLORATORY] Question 5: Finding Your Strategy for IPOs\n",
    "\n",
    "You've seen in the first questions that the median and average investments are negative in IPOs, and you can't blindly invest in all deals.\n",
    "\n",
    "How would you correct/refine the approach? Briefly describe the steps and the data you'll try to get (it should be generally feasible to do it from public sources - no access to internal data of companies)?\n",
    "\n",
    "E.g. (some ideas) Do you want to focus on the specific vertical? Do you want to build a smart comparison vs. existing stocks on the market? Or you just will want to get some features (which features?) like total number of people in a company to find a segment of \"successful\" IPOs?\n",
    "\n",
    "---\n",
    "## Submitting the solutions\n",
    "\n",
    "Form for submitting: https://courses.datatalks.club/sma-zoomcamp-2024/homework/hw02\n",
    "\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
